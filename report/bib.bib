@article{Fox2001,
abstract = {Over the last years, particle filters have been applied$\backslash$nwith great success to a variety of state estimation$\backslash$nproblems. We present a statistical approach to increasing$\backslash$nthe efficiency of particle filters by adapting the size of$\backslash$nsample sets on-the-fly. The key idea of the KLD-sampling$\backslash$nmethod is to bound the approximation error introduced by$\backslash$nthe sample-based representation of the particle filter. The$\backslash$nname KLD-sampling is due to the fact that we measure the$\backslash$napproximation error by the Kullback-Leibler distance. Our$\backslash$nadaptation approach chooses a small number of samples if$\backslash$nthe density is focused on a small part of the state space,$\backslash$nand it chooses a large number of samples if the state$\backslash$nuncertainty is high. Both the implementation and$\backslash$ncomputation overhead of this approach are small. Extensive$\backslash$nexperiments using mobile robot localization as a test$\backslash$napplication show that our approach yields drastic$\backslash$nimprovements over particle filters with fixed sample set$\backslash$nsizes and over a previously introduced adaptation$\backslash$ntechnique.},
author = {Fox, Dieter},
doi = {10.1.1.21.5786},
isbn = {0262042088},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {Robotics,Robotics/Localisation},
pmid = {19806183},
title = {{KLD-sampling: Adaptive particle filters and mobile robot localization}},
year = {2001}
}
@article{Labbe2018,
abstract = {Distributed as an open‐source library since 2013, real‐time appearance‐based mapping (RTAB‐Map) started as an appearance‐based loop closure detection approach with memory management to deal with large‐scale and long‐term online operation. It then grew to implement simultaneous localization and mapping (SLAM) on various robots and mobile platforms. As each application brings its own set of constraints on sensors, processing capabilities, and locomotion, it raises the question of which SLAM approach is the most appropriate to use in terms of cost, accuracy, computation power, and ease of integration. Since most of SLAM approaches are either visual‐ or lidar‐based, comparison is difficult. Therefore, we decided to extend RTAB‐Map to support both visual and lidar SLAM, providing in one package a tool allowing users to implement and compare a variety of 3D and 2D solutions for a wide range of applications with different robots and sensors. This paper presents this extended version of RTAB‐Map and its use in comparing, both quantitatively and qualitatively, a large selection of popular real‐world datasets (e.g., KITTI, EuRoC, TUM RGB‐D, MIT Stata Center on PR2 robot), outlining strengths, and limitations of visual and lidar SLAM configurations from a practical perspective for autonomous navigation applications.},
author = {Labb{\'{e}}, M. and Michaud, F.},
doi = {10.1002/rob.21831},
file = {:home/lunarpulse/Downloads/Labbe18JFR{\_}preprint.pdf:pdf},
issn = {15564959},
journal = {Journal of Field Robotics},
mendeley-groups = {Robotics/SLAM},
title = {{RTAB-Map as an open-source lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation}},
url = {http://doi.wiley.com/10.1002/rob.21831},
year = {2018}
}

@article{Thrun2002,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Thrun, Sebastian},
doi = {10.1145/504729.504754},
eprint = {arXiv:1011.1669v3},
file = {:home/lunarpulse/Downloads/ProbabilisticRobotics.pdf:pdf},
isbn = {9788578110796},
issn = {00010782},
journal = {Communications of the ACM},
mendeley-groups = {Robotics,Robotics/Localisation,Robotics/SLAM},
pages = {1999--2000},
pmid = {25246403},
title = {{Probabilistic robotics}},
year = {2002}
}

@misc{Missri,
author = {Missri, Salah},
mendeley-groups = {Robotics/Collision Avoidance/RGB-D},
title = {{SyrianSpock/realsense{\_}gazebo{\_}plugin: Intel RealSense R200 Gazebo ROS plugin and model}},
url = {https://github.com/SyrianSpock/realsense{\_}gazebo{\_}plugin},
urldate = {2018-11-04}
}

@book{Joseph2015,
abstract = {Mastering ROS for Robotics Programming is an advanced guide of ROS that is very suitable for readers who already have a basic knowledge in ROS. ROS is widely used in robotics companies, universities, and robotics research institutes for designing, building, and simulating a robot model and interfacing it into real hardware. ROS is now an essential requirement for Robotic engineers; this guide can help you acquire knowledge of ROS and can also help you polish your skills in ROS using interactive examples. Even though it is an advanced guide, you can see the basics of ROS in the first chapter to refresh the concepts. It also helps ROS beginners. The book mainly focuses on the advanced concepts of ROS, such as ROS Navigation stack, ROS MoveIt!, ROS plugins, nodelets, controllers, ROS Industrial, and so on. You can work with the examples in the book without any special hardware; however, in some sections you can see the interfacing of I/O boards, vision sensors, and actuators to ROS. To work with this hardware, you will need to buy it. The book starts with an introduction to ROS and then discusses how to build a robot model in ROS for simulating and visualizing. After the simulation of robots using Gazebo, we can see how to connect the robot to Navigation stack and MoveIt!. In addition to this, we can see ROS plugins, controllers, nodelets, and interfacing of I/O boards and vision sensors. Finally, we can see more about ROS Industrial and troubleshooting and best practices in ROS.},
address = {Birmingham},
author = {Joseph, Lentin},
file = {:home/lunarpulse/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joseph - 2015 - Mastering ROS for Robotics Programming.pdf:pdf},
isbn = {9781783551798},
keywords = {ROS},
mendeley-groups = {Robotics/ROS},
mendeley-tags = {ROS},
pages = {451},
publisher = {Packt Publishing Ltd},
title = {{Mastering ROS for Robotics Programming}},
year = {2015}
}

@misc{AMCL_ROS,
mendeley-groups = {Robotics/Localisation},
title = {{amcl - ROS Wiki}},
url = {http://wiki.ros.org/amcl{\#}Parameters},
urldate = {2018-11-04}
}

@misc{Maturana2015c,
author = {Maturana, D and Scherer, S},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2015.7353481},
file = {:home/lunarpulse/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maturana, Scherer - 2015 - VoxNet A 3D Convolutional Neural Network for real-time object recognition.pdf:pdf},
isbn = {21530858},
mendeley-groups = {NeuralNetwork/CNN},
pages = {922--928},
title = {{VoxNet: A 3D Convolutional Neural Network for real-time object recognition}},
volume = {2015-},
year = {2015}
}
@article{Mirowski2016,
abstract = {Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour, its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.},
author = {Mirowski, Piotr and Pascanu, Razvan and Viola, Fabio and Soyer, Hubert and Ballard, Andrew J and Banino, Andrea and Denil, Misha and Goroshin, Ross and Sifre, Laurent and Kavukcuoglu, Koray and Kumaran, Dharshan and Hadsell, Raia},
file = {:home/lunarpulse/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mirowski et al. - 2016 - Learning to Navigate in Complex Environments.pdf:pdf},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision And Pattern Rec,Computer Science - Learning,Computer Science - Robotics,Deep learning,RL,Reinforcement learning,stacked LSTM},
mendeley-groups = {NeuralNetwork/DRNN,Artificial intel},
mendeley-tags = {Deep learning,RL,Reinforcement learning,stacked LSTM},
title = {{Learning to Navigate in Complex Environments}},
year = {2016}
}
@article{Mnih2015a,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei a and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:home/lunarpulse/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
mendeley-groups = {Artificial intel},
number = {7540},
pages = {529--533},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {518},
year = {2015}
}
@book{Russell1995,
abstract = {Hepatocellular carcinoma (HCC) used to be considered a universally fatal disease. Today, however, we have tools to identify patients at risk for HCC with more accuracy. We are able to provide surveillance using ultrasonography that is sufficiently sensitive to detect small HCC lesions. Treatment of these lesions, whether by resection or by radiofrequency ablation, is highly effective. These advances mean that HCC is theoretically curable in the majority of patients, provided these tools are used. Microarray technology has been applied to the study of the genetic changes in HCC, and has defined several distinct genetic variants of this disease, as well as identifying gene signatures that predict poor outcome, and predict metastases. These techniques are now being used to identify new potential targets for therapy, and hold great promise for the future.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Russell, Stuart J and Norvig, Peter and Canny, John F and Malik, Jitendra M and Edwards, Douglas D and Jonathan, Stuart J Stuart},
booktitle = {Theory and Practice},
doi = {10.1007/s11894-010-0163-7},
eprint = {9809069v1},
file = {:home/lunarpulse/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Russell et al. - 1995 - A Modern Approach.pdf:pdf},
isbn = {0137903952},
issn = {1534312X},
mendeley-groups = {Artificial intel,NeuralNetwork},
number = {2},
pages = {106--10},
pmid = {21104209},
primaryClass = {arXiv:gr-qc},
title = {{A Modern Approach}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21104209},
volume = {13},
year = {1995}
}
@article{Lillicrap2015,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
doi = {10.1561/2200000006},
eprint = {1509.02971},
file = {:home/lunarpulse/Downloads/1509.02971.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
mendeley-groups = {NeuralNetwork/reinforcement learning,Artificial intel},
pmid = {24966830},
title = {{Continuous control with deep reinforcement learning}},
url = {http://arxiv.org/abs/1509.02971},
year = {2015}
}
@article{Even-Dar2002,
abstract = {We show the convergence of two deterministic variants of Q-learning. The first is the widely used optimistic Q-learning, which initializes the Q-values to large initial values and then follows a greedy policy with respect to the Q-values. We show that setting the initial value sufficiently large guarantees the converges to an epsilon-optimal policy. The second is a new and novel algorithm incremental Q-learning, which gradually promotes the values of actions that are not taken. We show that incremental Q-learning converges, in the limit, to the optimal policy. Our incremental Q-learning algorithm can be viewed as derandomization of the epsilon-greedy Q-learning.},
author = {Even-Dar, Eyal and Mansour, Yishay},
file = {:home/lunarpulse/Downloads/1944-convergence-of-optimistic-and-incremental-q-learning.pdf:pdf},
isbn = {1049-5258},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 14},
keywords = {stochastic-approximation},
mendeley-groups = {Artificial intel},
pages = {1499--1506$\backslash$r1594},
title = {{Convergence of optimistic and incremental Q-learning}},
url = {http://papers.nips.cc/paper/1944-convergence-of-optimistic-and-incremental-q-learning.pdf{\%}0Ahttps://papers.nips.cc/paper/1944-convergence-of-optimistic-and-incremental-q-learning},
volume = {14},
year = {2002}
}
@article{Plappert2018,
abstract = {The purpose of this technical report is two-fold. First of all, it introduces a suite of challenging continuous control tasks (integrated with OpenAI Gym) based on currently existing robotics hardware. The tasks include pushing, sliding and pick {\&} place with a Fetch robotic arm as well as in-hand object manipulation with a Shadow Dexterous Hand. All tasks have sparse binary rewards and follow a Multi-Goal Reinforcement Learning (RL) framework in which an agent is told what to do using an additional input. The second part of the paper presents a set of concrete research ideas for improving RL algorithms, most of which are related to Multi-Goal RL and Hindsight Experience Replay.},
archivePrefix = {arXiv},
arxivId = {1802.09464},
author = {Plappert, Matthias and Andrychowicz, Marcin and Ray, Alex and McGrew, Bob and Baker, Bowen and Powell, Glenn and Schneider, Jonas and Tobin, Josh and Chociej, Maciek and Welinder, Peter and Kumar, Vikash and Zaremba, Wojciech},
doi = {92-2-113360-5},
eprint = {1802.09464},
file = {:home/lunarpulse/Downloads/1802.09464.pdf:pdf},
isbn = {9789221263258},
mendeley-groups = {Artificial intel},
pages = {1--16},
title = {{Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research}},
url = {http://arxiv.org/abs/1802.09464},
year = {2018}
}
@article{Andrychowicz2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1707.01495v3},
author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and Mcgrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
eprint = {arXiv:1707.01495v3},
file = {:home/lunarpulse/Downloads/1707.01495.pdf:pdf},
mendeley-groups = {Artificial intel},
number = {Nips},
title = {{Hindsight Experience Replay}},
year = {2017}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
doi = {10.1016/j.jdeveco.2016.04.001},
eprint = {1707.06347},
file = {:home/lunarpulse/Downloads/1707.06347.pdf:pdf},
isbn = {0304-3878},
issn = {03043878},
mendeley-groups = {Artificial intel},
pages = {1--12},
pmid = {15694853},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@inproceedings{Fu2016,
author = {Fu, Michael C},
booktitle = {Proceedings of the 2016 Winter Simulation Conference},
file = {:home/lunarpulse/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fu - 2016 - AlphaGo and Monte Carlo tree search the simulation optimization perspective.pdf:pdf},
isbn = {1509044841},
mendeley-groups = {NeuralNetwork},
pages = {659--670},
publisher = {IEEE Press},
title = {{AlphaGo and Monte Carlo tree search: the simulation optimization perspective}},
year = {2016}
}
@misc{Hassabis2017,
abstract = {Artificial intelligence research has made rapid progress in a wide variety of domains from speech recognition and image classification to genomics and drug discovery. In many cases, these are specialist systems that leverage enormous amounts of human expertise and data.},
author = {Hassabis, Demis and Silver, David},
booktitle = {Deep Mind},
file = {:home/lunarpulse/Downloads/agz{\_}unformatted{\_}nature.pdf:pdf},
mendeley-groups = {Artificial intel},
title = {{AlphaGo Zero: Learning from scratch}},
year = {2017}
}
@article{Sabour2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1710.09829v2},
author = {Sabour, Sara and Nov, C V and Hinton, Geoffrey E},
eprint = {arXiv:1710.09829v2},
file = {:home/lunarpulse/Downloads/1710.09829.pdf:pdf},
mendeley-groups = {NeuralNetwork/Capsule},
number = {Nips},
title = {{Dynamic Routing Between Capsules}},
year = {2017}
}
@article{Szegedy2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1512.00567v3},
author = {Szegedy, Christian and Vanhoucke, Vincent and Shlens, Jonathon and Wojna, Zbigniew},
eprint = {arXiv:1512.00567v3},
file = {:home/lunarpulse/Downloads/1512.00567.pdf:pdf},
mendeley-groups = {NeuralNetwork/CNN},
title = {{Rethinking the Inception Architecture for Computer Vision}},
year = {2014}
}
