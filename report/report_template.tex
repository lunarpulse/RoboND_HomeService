
\documentclass[10pt,journal,compsoc]{IEEEtran}
\usepackage{listings}   
\usepackage[pdftex]{graphicx}    
\usepackage{cite}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
	
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstset{ 
      backgroundcolor=\color{backcolour},   
      commentstyle=\color{codegreen},
      keywordstyle=\color{magenta},
      numberstyle=\tiny\color{codegray},
      stringstyle=\color{codepurple},
      basicstyle=\footnotesize,
      breakatwhitespace=false,         
      breaklines=true,                 
      captionpos=b,                    
      keepspaces=true,                 
      numbers=left,                    
      numbersep=5pt,                  
      showspaces=false,                
      showstringspaces=false,
      showtabs=false,                  
      tabsize=2,
      deletekeywords={...},            % if you want to delete keywords from the given language
      escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
      extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
      frame=single,	                   % adds a frame around the code
      morekeywords={*,...},            % if you want to add more keywords to the set
      rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
      stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
      title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

\title{Dog Bread Identifier: Robotics Inference}

\author{Jung, Myoungki}

\markboth{Inference project, Robotics Nanodegree Program, Udacity}%
{}
\IEEEtitleabstractindextext{%

\begin{abstract}
Inference has been on a big trend since neral network suggested new paradigm in compute vision. The accuracy and versitility of new trend, neural network influenced many different field of machine learning and allow developers create various filters at ease.
This project shows how easy to make a inference filte from training a neural networks.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
IEEEtran, Udacity, CNN, Inference.
\end{IEEEkeywords}}

\maketitle
\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle
\section{Introduction}
\label{sec:introduction}

\IEEEPARstart{M}{dern} neural networks reignited machine learning provided many possibilities to robotics. Robotics inference is a key topic for robotics for operating tasks by a robot itself. Every task starts from recognising the surroundings first. Therefore, the accuracy of inference and latency of it is important for robotics field.

\section{Background}
There are two different neural networks were chosen to train the dataset.
The required \verb!P1_DATA! is a simple data structure and this could be solved by \verb!AlexNet! and my custom dataset has more complicated and versatile features requires more sophisticated \verb!GoogleLeNet! to be the trainer for the dataset.
The motivation of my dataset is to classify the breed of the dog by infering a photo of dog.

\section{Data Acquisition}

\subsection{Provided DATA}

The \verb!P1_DATA! was provided by Udacity and it has around 10 thousand photos of 3 categories.
\subsection{Dog Bread Identifier}

The \verb!P1_DATA! was provided by Udacity and it has around 9 hundred photos of 12 categories. This smaller data reduces the resultant model smaller than others.

\subsubsection{Dataset Download}

The data set was scrapped from google image search. Using python script shown in the listing \ref{list:DownloadDataSet}.

\begin{lstlisting}[language=Python, caption={Python script to download dataset },label={list:DownloadDataSet}]
#!/usr/bin/python
from apiclient.discovery import build
import imghdr
import os
import sys
import urllib2
from google_images_download import google_images_download

orig_query = raw_input("Please input the text document path containing the desired image queries: ")
pet_type = orig_query[:-5]
query = orig_query.replace(' ', '_')
pets = [line.rstrip('\r\n') for line in open(query)]
type_pets =["\""+pet+"\"" for pet in pets]

args_pets = ','.join(type_pets)
arg_pets = ','.join(pets)

response = google_images_download.googleimagesdownload()   #class instantiation
arguments = {"keywords":args_pets,"format":"png","chromedriver":"/usr/lib/chromium-browser/chromedriver","limit":200,"size":"medium","aspect_ratio":"square","print_urls":False}   #creating list of arguments
paths = response.download(arguments)
print(paths)
\end{lstlisting}
This script uses \verb!chromedriver! to search, parse, download and save the google image search. There is a commercial google api for 1000 images download per five dollars. However, this script does the same function for free. Python package \verb!google_images_download! is required to install prior to excute the script.


\subsubsection{Dataset Preparation}
Initially, 10 thousands for 130 different dog speices were prepared and it was reduced to data sample which is a portrait image of a dog with distinctive looks in order to help training. Noises in samples, multiple objects in the scene, unrelated objects in the scene, characters, and so on, did affect overall performance of network as these confuses the network back propagation functions and oscilates the loss values.
Both udacity provided dataset and the my own dataset was divided to train, validation, test data set with ratio of 75:20:5. Table  \ref{table:CustomDatasetInfo} shows the common details of both dataset, and the only difference is the \verb!Dataset size!, 85.4 MB for my own dataset, and 930 MB.

\begin{lstlisting}[language=Python, caption={Python script to resize and rename the file and directory name of dataset},label={list:RenameResizeDataSet}]
#!/usr/bin/python
from PIL import Image
import os, sys, re
path = "~/Projects/Dataset/"
dirs = os.listdir( path )

def count_em(path):
      for root, dirs, files in sorted(os.walk(path)):
            for file_ in files:
            full_file_path = os.path.join(root, file_)
            print (full_file_path)
            try:
                  img = Image.open(full_file_path)
                  new_width  = 255 
                  new_height = 255
                  img = img.resize((new_width, new_height), Image.ANTIALIAS)
                  img.save(os.path.join(root, file_+''),'png')
            except IOError, ex:
                  os.remove(full_file_path)

count_em(path)      
\end{lstlisting}
After this step, archived data set was uploaded to udacity workspace and unzipped to create dataset database in DIGITS.

\begin{table}[ht]
      \caption{Dataset in DIGITS}
      \label{table:CustomDatasetInfo}
      \begin{center}
      \begin{tabular}{|c|c|}
      \hline
      Parameter & Value \\
      \hline\hline
      \verb!Image Dimensions! & 256x256 (Width x Height)\\
      \hline
      \verb!Image Type! & Color\\
      \hline
      \verb!Resize Transformation! & Squash\\
      \hline
      \verb!DB Backend! & lmdb\\
      \hline
      \verb!Image Encoding! & png\\
      \hline
      \verb!DB Compression! & none\\
      \hline
      \end{tabular}
      \end{center}
\end{table}

\section{Hyperparameters}
\subsection{Training Hyperparameters}
The difference is the larger number of training epochs, due to the smaller amount of dataset compared to the other.
A policy to change the learning rate was applied to both training instaces as shown bottom of each Table. This helped to stablise the loss function.

\subsubsection{Udacity Dataset Training}

Training Hyperparameter was set in 'New Image Classification Model' section of DIGITS site. The Table \ref{table:Udacity Dataset Training Hyperparameters} shows the used hyperparameters to train the \verb!AlexNet! for the provided dataset.
\begin{table}[ht]
      \caption{Details of Udacity Dataset Training Hyperparameters }
      \label{table:Udacity Dataset Training Hyperparameters}
      \begin{center}
      \begin{tabular}{|c|c|}
      \hline
      Parameter & Value \\
      \hline\hline
      \verb!Training epochs ! & 30\\
      \hline
      \verb!Snapshot interval ! & every 1 epoch\\
      \hline
      \verb!Validation interval! & every 1 epoch\\
      \hline
      \verb!Random seed ! & None\\
      \hline
      \verb!Batch size! & None\\
      \hline
      \verb!Solver type ! & AdaGrad\\
      \hline
      \verb!Base Learning Rate! & 0.005\\
      \hline
      \verb!Solver type ! & AdaGrad\\
      \hline
      \verb!Policy! & stepdown\\
      \hline
      \verb!Step Size! & 0.33\\
      \hline
      \verb!Gamma! & 0.5\\
      \hline
      \verb!Network! & AlexNet\\
      \hline
      \end{tabular}
      \end{center}
\end{table}
As the dataset has three categories and the amount of data is large only 30 epochs could meet the project requirement, an inference accuracy more than 75 percents.

\subsubsection{Dog Breed Dataset Training}
The hyperparameters for GoogleLeNet network for dog breed identifier network was set as Table \ref{table:Dog Breed Dataset Training Hyperparameters}.
\begin{table}[ht]
      \caption{Details of Dog Breed Dataset Training Hyperparameters }
      \label{table:Dog Breed Dataset Training Hyperparameters}
      \begin{center}
      \begin{tabular}{|c|c|}
      \hline
      Parameter & Value \\
      \hline\hline
      \verb!Training epochs ! & 75\\
      \hline
      \verb!Snapshot interval ! & every 1 epoch\\
      \hline
      \verb!Validation interval! & every 1 epoch\\
      \hline
      \verb!Random seed ! & None\\
      \hline
      \verb!Batch size! & None\\
      \hline
      \verb!Solver type ! & AdaGrad\\
      \hline
      \verb!Base Learning Rate! & 0.005\\
      \hline
      \verb!Solver type ! & AdaGrad\\
      \hline
      \verb!Policy! & stepdown\\
      \hline
      \verb!Step Size! & 0.33\\
      \hline
      \verb!Gamma! & 0.5\\
      \hline
      \verb!Network! & GoogLeNet \cite{Szegedy2014}\\
      \hline
      \end{tabular}
      \end{center}
\end{table}

\section{Results}
\subsection{AlexNet for Udacity Dataset}

Figure \ref{fig:Udacity30Epoch} shows the enitre training progressof the AlexNet for provided dataset.
\begin{figure}
	\begin{subfigure}{0.235\textwidth}
            \centering
            \includegraphics[width=0.95\linewidth]{./img/ProvidedALEXNET/P1_model_cls_ggle_re.png}
            \caption{First 15 Epoches of training with starting learning rate 0.005}
            \label{fig:UdacityFirst15Epoch}
      \end{subfigure}%
	\begin{subfigure}{0.235\textwidth}
            \includegraphics[width=0.95\linewidth]{./img/ProvidedALEXNET/P1_model_cls_ggle_reLR00125ADAM.png}
            \caption{Second 15 Epoches with starting learning rate 0.00125}
            \label{fig:UdacitySecond15Epoch}
      \end{subfigure}
      
	\caption{Analysis for run4}
	\label{fig:Udacity30Epoch}
 \end{figure}
Figure \ref{fig:UdacityEvaluationResult} indicates that the result for evaluation using 'evaluate' command on a terminal passes all the requirements for the project.
\begin{figure}[thpb]
      \centering
      \includegraphics[width=\linewidth]{./img/ProvidedALEXNET/evaluationResult.png}
      \caption{Agent reaching reaching with gripper}
      \label{fig:UdacityEvaluationResult}
\end{figure}
The evaluation result, 5 ms inference and 75.3 percent accuracy, on Figure \ref{fig:UdacityEvaluationResult} exceeds the project requirement, under 10ms inference, and 75 percent accuracy.

\subsection{Dog Breed Identifier Network}

The first overall 80 percent of touching the tube with gripper is shown on Figure \ref{fig:DogIdentifierGGLNET}.
\begin{figure}[thpb]
      \centering
      \includegraphics[width=\linewidth]{./img/DogIdentifierGGLNET/DogBreedIdentifierReducedCleanedGGLNprLR005.png}
      \caption{Result of Dog Breed Identifier Network Training}
      \label{fig:DogIdentifierGGLNET}
\end{figure}

\begin{figure}[thpb]
      \begin{subfigure}[b]{0.23\textwidth}
              \includegraphics[width=0.95\linewidth]{./img/DogIdentifierGGLNET/bc99p.png}
              \caption{Border Collie:0.99}
              \label{fig:Infer_bc99}
      \end{subfigure}%
      \begin{subfigure}[b]{0.23\textwidth}
            \includegraphics[width=0.95\linewidth]{./img/DogIdentifierGGLNET/greyhound99p.png}
            \caption{Grey Hound: 0.99}
            \label{fig:Infer_grh99}
      \end{subfigure}
      \begin{subfigure}[b]{0.23\textwidth}
            \includegraphics[width=0.95\linewidth]{./img/DogIdentifierGGLNET/germansp99p.png}
            \caption{German Sheperd: 0.99}
            \label{fig:Infer_gs99}
      \end{subfigure}%
      \begin{subfigure}[b]{0.23\textwidth}
            \includegraphics[width=0.95\linewidth]{./img/DogIdentifierGGLNET/aussiecattledog96p.png}
            \caption{Kelpie: 0.99}
            \label{fig:Infer_ke99}
      \end{subfigure}
      \begin{subfigure}[b]{0.23\textwidth}
            \includegraphics[width=0.95\linewidth]{./img/DogIdentifierGGLNET/95percent.png}
            \caption{Collie: 0.95}
            \label{fig:Infer_co96}
      \end{subfigure}%
      \begin{subfigure}[b]{0.23\textwidth}
            \includegraphics[width=0.95\linewidth]{./img/DogIdentifierGGLNET/nr71p.png}
            \caption{Norwich Terrier: 0.71}
            \label{fig:nl71}
      \end{subfigure}
      \caption{High accuracy of inference from the trained network}\label{fig:High accuracy of results from trained network}
\end{figure}
Because there is no automated script to test the performance of the trained network, the inference accuracy with manual random inference check shows the high accuracy of trained network, Figure \ref{fig:High accuracy of results from trained network} and poor inference results \ref{fig:Poor accuracy of inference from the trained network}.

\begin{figure}[thpb]
      \begin{subfigure}[b]{0.23\textwidth}
            \includegraphics[width=0.95\linewidth]{./img/DogIdentifierGGLNET/understands.png}
            \caption{Understand but not sure}
            \label{fig:understand}
      \end{subfigure}%
      \begin{subfigure}[b]{0.23\textwidth}
              \includegraphics[width=0.95\linewidth]{./img/DogIdentifierGGLNET/notevenlisted.png}
              \caption{Not even listed}
              \label{fig:notlisted}
      \end{subfigure} 
      
      \caption{Poor accuracy of inference from the trained network}\label{fig:Poor accuracy of inference from the trained network}
\end{figure}
 
The project related files listed below are archived into tar.gz or zip files.

\begin{itemize}
      \item \verb!deploy.prototxt!
      \item \verb!labels.txt!
      \item \verb!mean.binaryproto!
      \item \verb!your_model.caffemodel!
      \item \verb!solver.prototxt!
      \item \verb!train_val.prototxt!
\end{itemize}

Dog breed identifier network related are archived as:
\begin{itemize}
      \item \verb!DogBreedIdentifierReducedCleanedGGLNprLR005/20181209-120658-78da_epoch_75.0.tar.gz! : contains the epoch 51 model configuration for dog breed inference network.
      \item \verb!DogBreedIdentifierReducedCleanedGGLNprLR005/20181209-120658-78da_epoch_75.0.tar.gz! : contains the epoch 51 model configuration for dog breed inference network.
      \item \verb!DogBreedIdentifierReducedCleanedGGLNprLR005/dogImagesReduced.zip! : Reduced Dataset (only 12 categories)
\end{itemize}
The archive files related to the classification network for Udacity provided data, \verb!P1_DATA!, are:
\begin{itemize}
      \item \verb!providedModel75p/20181209-143859-8ecf_epoch_15.0.tar.gz! : contains the epoch 30 of model configuration for the classification network for Udacity provided data.
\end{itemize}

\section{Discussion}
Obtaining approperiate dataset was the hardest start. The dataset must be diverse and not containing too many unrelated objects and the larger  amount of it is better training result.
Poor inference also inherits from the biased data, or not well prepared data. Some postures of the dogs in the datasample is rather fewer than other posture and these rare samples limits the accuracy if a trained network is tested with such less experienced test dataset. In other words, dataset for training should contain as many cases in variety and amount as possible.
Training the neural networks for the models took a great deal of attention. The networks were well pre-defined by default, however, the training epochs and learning rate affected its stability and accuracy significantly. From many trials, learning rate 0.005 was a good starting point which lifts the training accuracy of the network to 90 percent within 10 epochs and learning rate lower than that value resulted in low overall training accuracy by not applying the loss to the network well, especially with the policy of reduction of a learning rate. With high value of learning rate more than 0.075, the loss and accuracy oscilates in later epochs did not reach high accuracy after a training.
 \section{Conclusion / Future work}
The project 
The inference using name networks performs with an adequate dataset and training. The complicated convolutional networks perform well in many cases, still many other new architectures are introduced. CapsNet \cite{Sabour2017} utilises dynamic routing algorithm outperforms CNN in terms of accuracy and inference completeness for entire layout of the inference images.
The framework usded in the DIGITS is caffee and its syntax is different from well known frameworks such as \verb!tensorflow!. If porting to online inference on a robot is considered, \verb!Tensorflow RT! is suggested as NVIDIA Jetson embraces this framework to utilise their CUDA processors optimally.
Inference tasks in an active research field and there will be more better networks coming and the developers have to keep watching the trend to implment the greatness of the techonology.
 \bibliography{bib}
\bibliographystyle{ieeetr}

\end{document}
